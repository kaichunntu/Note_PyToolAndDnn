{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "ly = tf.contrib.layers\n",
    "\n",
    "import pickle as pk\n",
    "import multiprocessing as mp\n",
    "# import skimage.io as image_io\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from utils.process_func import *\n",
    "\n",
    "sess_opt = tf.ConfigProto(gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95 , allow_growth=True)\n",
    "                         ,allow_soft_placement=True\n",
    "#                          ,log_device_placement=True\n",
    "                         ,device_count={'GPU': 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Dataset is [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000, 3072)\n",
      "(10000, 3072)\n",
      "(10000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "datas = []\n",
    "labels = []\n",
    "for i in range(5):\n",
    "    train_data = unpickle('cifar-10-batches-py/data_batch_'+str(i+1))\n",
    "    print(train_data[b'data'].shape)\n",
    "    #print(train_data[b'labels'].shape)\n",
    "    #print(train_data.keys())\n",
    "    #print(train_data[b'data'])\n",
    "    if i == 0:\n",
    "        datas.append(train_data[b'data']/255)\n",
    "        labels.append(train_data[b'labels'])\n",
    "    else:\n",
    "        datas.append(train_data[b'data']/255)\n",
    "        labels.append(train_data[b'labels'])\n",
    "datas = np.concatenate(datas).reshape(-1,3,32,32).transpose(0,2,3,1)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:-1 * valid_data_size], Y_all[0:-1 * valid_data_size]\n",
    "    X_valid, Y_valid = X_all[-1 * valid_data_size:], Y_all[-1 * valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x , train_y , val_x , val_y = split_valid_set(datas,labels,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x,gpu_idx):\n",
    "    if gpu_idx == 0:\n",
    "        reuse_flag = False\n",
    "    else:\n",
    "        reuse_flag = True\n",
    "    \n",
    "    with tf.name_scope(\"extract_feat_%s\"%gpu_idx),tf.variable_scope(\"model\",reuse=reuse_flag):\n",
    "        x = ly.conv2d(x,16,3,1,scope=\"conv2d_0\")\n",
    "        x = ly.conv2d(x,32,5,1,scope=\"conv2d_1\")\n",
    "        x = ly.max_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.conv2d(x,64,3,1,scope=\"conv2d_2\")\n",
    "        x = ly.conv2d(x,32,3,1,scope=\"conv2d_3\")\n",
    "        x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.conv2d(x,64,3,1,scope=\"conv2d_4\")\n",
    "        x = ly.conv2d(x,32,3,1,scope=\"conv2d_5\")\n",
    "        x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.flatten(x)\n",
    "        x = ly.fully_connected(x,256,scope=\"fc_0\")\n",
    "        x = ly.fully_connected(x,128,scope=\"fc_1\")\n",
    "        x = ly.fully_connected(x,10,scope=\"fc_2\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1(x,gpu_idx):\n",
    "    if gpu_idx == 0:\n",
    "        reuse_flag = False\n",
    "    else:\n",
    "        reuse_flag = True\n",
    "    \n",
    "    with tf.name_scope(\"extract_feat_%s\"%gpu_idx),tf.variable_scope(\"model\",reuse=reuse_flag):\n",
    "        x = ly.conv2d(x,16,3,1,scope=\"conv2d_0\")\n",
    "        x = ly.conv2d(x,32,5,1,scope=\"conv2d_1\")\n",
    "        x = ly.max_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.conv2d(x,64,3,1,scope=\"conv2d_2\")\n",
    "        x = ly.conv2d(x,32,3,1,scope=\"conv2d_3\")\n",
    "        x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.conv2d(x,64,3,1,scope=\"conv2d_4\")\n",
    "        x = ly.conv2d(x,32,3,1,scope=\"conv2d_5\")\n",
    "        x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "#         x = ly.flatten(x)\n",
    "#         x = ly.fully_connected(x,256,scope=\"fc_0\")\n",
    "#         x = ly.fully_connected(x,128,scope=\"fc_1\")\n",
    "#         x = ly.fully_connected(x,10,scope=\"fc_2\")\n",
    "    \n",
    "    return x\n",
    "def build_model_2(x,gpu_idx):\n",
    "    if gpu_idx == 0:\n",
    "        reuse_flag = False\n",
    "    else:\n",
    "        reuse_flag = True\n",
    "    \n",
    "    with tf.name_scope(\"extract_feat_%s\"%gpu_idx),tf.variable_scope(\"model\",reuse=False):\n",
    "#         x = ly.conv2d(x,16,3,1,scope=\"conv2d_0\")\n",
    "#         x = ly.conv2d(x,32,5,1,scope=\"conv2d_1\")\n",
    "#         x = ly.max_pool2d(x,[3,3])\n",
    "\n",
    "#         x = ly.conv2d(x,64,3,1,scope=\"conv2d_2\")\n",
    "#         x = ly.conv2d(x,32,3,1,scope=\"conv2d_3\")\n",
    "#         x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "#         x = ly.conv2d(x,64,3,1,scope=\"conv2d_4\")\n",
    "#         x = ly.conv2d(x,32,3,1,scope=\"conv2d_5\")\n",
    "#         x = ly.avg_pool2d(x,[3,3])\n",
    "\n",
    "        x = ly.flatten(x)\n",
    "        x = ly.fully_connected(x,256,scope=\"fc_0\")\n",
    "        x = ly.fully_connected(x,128,scope=\"fc_1\")\n",
    "        x = ly.fully_connected(x,10,scope=\"fc_2\")\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below refers to [cifar10_multi_gpu_train.py](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py)\n",
    " written by Vijay Vasudevan, Manjunath Kudlur, josh11b, Daniel Smilkov, Derek Murray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat( grads , axis=0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build...\n"
     ]
    }
   ],
   "source": [
    "cifar_g = tf.Graph()\n",
    "with cifar_g.as_default() as g , tf.device(\"/gpu:0\"):\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        img = tf.placeholder(tf.float32,[None,32,32,3])\n",
    "        ys = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        split_img = tf.split(img,2)\n",
    "        split_label = tf.split(ys,2)\n",
    "        \n",
    "    with tf.name_scope(\"train_optimize\"):\n",
    "        optimizer = tf.train.RMSPropOptimizer(5e-4)\n",
    "        \n",
    "    score_list = []\n",
    "    loss_list = []\n",
    "    correct_count_list = []\n",
    "    gradients_list = []\n",
    "    for i in range(2):\n",
    "        with tf.device(\"/gpu:%s\"%i):\n",
    "            score = build_model(split_img[i],i)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=score,labels=split_label[i])\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            each_grads = optimizer.compute_gradients(loss)\n",
    "            gradients_list.append(each_grads)\n",
    "            \n",
    "            prob  = tf.nn.softmax(score,axis=-1)\n",
    "            acc = tf.argmax(prob,axis=-1,output_type=tf.int32)\n",
    "            acc = tf.cast(tf.equal(acc,split_label[i]),tf.float32)\n",
    "            \n",
    "            score_list.append(prob)\n",
    "            loss_list.append(loss)\n",
    "            correct_count_list.append(acc)\n",
    "            \n",
    "    \n",
    "    mean_loss = tf.reduce_mean(tf.stack(loss_list,axis=0))\n",
    "    mean_acc = tf.reduce_mean(tf.concat(correct_count_list,axis=0))\n",
    "    mean_grads = average_gradients(gradients_list)\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    apply_gradient_op = optimizer.apply_gradients(mean_grads)\n",
    "    \n",
    "    tf.summary.FileWriter(logdir=\"tblog/cifar\",graph=g)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "print(\"Build...\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=cifar_g,config=sess_opt)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iters  acc : 0.09375 loss : 2.3026052\n",
      "1 iters  acc : 0.0546875 loss : 2.3027701\n",
      "2 iters  acc : 0.12109375 loss : 2.302414\n",
      "3 iters  acc : 0.09375 loss : 2.3025906\n",
      "4 iters  acc : 0.0703125 loss : 2.302817\n",
      "5 iters  acc : 0.109375 loss : 2.3024406\n",
      "6 iters  acc : 0.07421875 loss : 2.30271\n",
      "7 iters  acc : 0.0859375 loss : 2.3026068\n",
      "8 iters  acc : 0.0859375 loss : 2.30278\n",
      "9 iters  acc : 0.09375 loss : 2.3026814\n",
      "10 iters  acc : 0.1171875 loss : 2.3024611\n",
      "11 iters  acc : 0.109375 loss : 2.3025243\n",
      "12 iters  acc : 0.07421875 loss : 2.3027248\n",
      "13 iters  acc : 0.10546875 loss : 2.3025255\n",
      "14 iters  acc : 0.09375 loss : 2.3024888\n",
      "15 iters  acc : 0.078125 loss : 2.3027368\n",
      "16 iters  acc : 0.1015625 loss : 2.302608\n",
      "17 iters  acc : 0.0703125 loss : 2.302728\n",
      "18 iters  acc : 0.140625 loss : 2.302475\n",
      "19 iters  acc : 0.09765625 loss : 2.302594\n",
      "20 iters  acc : 0.14453125 loss : 2.3022618\n",
      "21 iters  acc : 0.09765625 loss : 2.3023696\n",
      "22 iters  acc : 0.14453125 loss : 2.3023667\n",
      "23 iters  acc : 0.078125 loss : 2.3027463\n",
      "24 iters  acc : 0.10546875 loss : 2.302545\n",
      "25 iters  acc : 0.05859375 loss : 2.3027892\n",
      "26 iters  acc : 0.12890625 loss : 2.3023672\n",
      "27 iters  acc : 0.08984375 loss : 2.3025227\n",
      "28 iters  acc : 0.11328125 loss : 2.3025458\n",
      "29 iters  acc : 0.10546875 loss : 2.3025365\n",
      "30 iters  acc : 0.09765625 loss : 2.3024945\n",
      "31 iters  acc : 0.09765625 loss : 2.3025455\n",
      "32 iters  acc : 0.109375 loss : 2.3025148\n",
      "33 iters  acc : 0.09765625 loss : 2.30266\n",
      "34 iters  acc : 0.08984375 loss : 2.302536\n",
      "35 iters  acc : 0.10546875 loss : 2.3025854\n",
      "36 iters  acc : 0.140625 loss : 2.3023043\n",
      "37 iters  acc : 0.12109375 loss : 2.30245\n",
      "38 iters  acc : 0.10546875 loss : 2.3023963\n",
      "39 iters  acc : 0.13671875 loss : 2.3022587\n",
      "40 iters  acc : 0.125 loss : 2.302249\n",
      "41 iters  acc : 0.109375 loss : 2.3025186\n",
      "42 iters  acc : 0.10546875 loss : 2.3023543\n",
      "43 iters  acc : 0.12109375 loss : 2.3022575\n",
      "44 iters  acc : 0.08984375 loss : 2.3025799\n",
      "45 iters  acc : 0.109375 loss : 2.3024359\n",
      "46 iters  acc : 0.15625 loss : 2.3019636\n",
      "47 iters  acc : 0.10546875 loss : 2.302629\n",
      "48 iters  acc : 0.12109375 loss : 2.3023148\n",
      "49 iters  acc : 0.0859375 loss : 2.3025513\n",
      "50 iters  acc : 0.12109375 loss : 2.3023067\n",
      "51 iters  acc : 0.109375 loss : 2.3023362\n",
      "52 iters  acc : 0.09375 loss : 2.3027067\n",
      "53 iters  acc : 0.12890625 loss : 2.3022704\n",
      "54 iters  acc : 0.09375 loss : 2.3024864\n",
      "55 iters  acc : 0.08984375 loss : 2.3028011\n",
      "56 iters  acc : 0.1171875 loss : 2.3023503\n",
      "57 iters  acc : 0.109375 loss : 2.3024993\n",
      "58 iters  acc : 0.13671875 loss : 2.302072\n",
      "59 iters  acc : 0.1015625 loss : 2.3025975\n",
      "60 iters  acc : 0.1015625 loss : 2.3021839\n",
      "61 iters  acc : 0.09765625 loss : 2.3025525\n",
      "62 iters  acc : 0.1171875 loss : 2.3023877\n",
      "63 iters  acc : 0.07421875 loss : 2.3027036\n",
      "64 iters  acc : 0.12109375 loss : 2.3021026\n",
      "65 iters  acc : 0.07421875 loss : 2.3025846\n",
      "66 iters  acc : 0.09375 loss : 2.3024902\n",
      "67 iters  acc : 0.140625 loss : 2.3018184\n",
      "68 iters  acc : 0.09375 loss : 2.3023682\n",
      "69 iters  acc : 0.12109375 loss : 2.3021414\n",
      "70 iters  acc : 0.08984375 loss : 2.3024197\n",
      "71 iters  acc : 0.109375 loss : 2.3024654\n",
      "72 iters  acc : 0.10546875 loss : 2.3025136\n",
      "73 iters  acc : 0.109375 loss : 2.3022258\n",
      "74 iters  acc : 0.1171875 loss : 2.3024771\n",
      "75 iters  acc : 0.12890625 loss : 2.3024015\n",
      "76 iters  acc : 0.10546875 loss : 2.3023868\n",
      "77 iters  acc : 0.1484375 loss : 2.3018904\n",
      "78 iters  acc : 0.1015625 loss : 2.3026505\n",
      "79 iters  acc : 0.1875 loss : 2.3015003\n",
      "80 iters  acc : 0.140625 loss : 2.3021274\n",
      "81 iters  acc : 0.14453125 loss : 2.3019972\n",
      "82 iters  acc : 0.140625 loss : 2.302194\n",
      "83 iters  acc : 0.12109375 loss : 2.3022866\n",
      "84 iters  acc : 0.09765625 loss : 2.3023052\n",
      "85 iters  acc : 0.13671875 loss : 2.3021288\n",
      "86 iters  acc : 0.140625 loss : 2.3022451\n",
      "87 iters  acc : 0.13671875 loss : 2.3019137\n",
      "88 iters  acc : 0.14453125 loss : 2.3019915\n",
      "89 iters  acc : 0.10546875 loss : 2.3022308\n",
      "90 iters  acc : 0.1328125 loss : 2.301724\n",
      "91 iters  acc : 0.140625 loss : 2.3018131\n",
      "92 iters  acc : 0.1484375 loss : 2.3017774\n",
      "93 iters  acc : 0.1171875 loss : 2.3018456\n",
      "94 iters  acc : 0.140625 loss : 2.3019536\n",
      "95 iters  acc : 0.1171875 loss : 2.3013833\n",
      "96 iters  acc : 0.12109375 loss : 2.3017068\n",
      "97 iters  acc : 0.1015625 loss : 2.301269\n",
      "98 iters  acc : 0.140625 loss : 2.3010054\n",
      "99 iters  acc : 0.0859375 loss : 2.3027585\n",
      "100 iters  acc : 0.14453125 loss : 2.3003616\n",
      "101 iters  acc : 0.09765625 loss : 2.3016174\n",
      "102 iters  acc : 0.09375 loss : 2.3021076\n",
      "103 iters  acc : 0.12890625 loss : 2.3018234\n",
      "104 iters  acc : 0.12109375 loss : 2.301961\n",
      "105 iters  acc : 0.09765625 loss : 2.3017488\n",
      "106 iters  acc : 0.16796875 loss : 2.3011327\n",
      "107 iters  acc : 0.09765625 loss : 2.301776\n",
      "108 iters  acc : 0.1640625 loss : 2.3010318\n",
      "109 iters  acc : 0.171875 loss : 2.3006454\n",
      "110 iters  acc : 0.17578125 loss : 2.299995\n",
      "111 iters  acc : 0.140625 loss : 2.300412\n",
      "112 iters  acc : 0.1015625 loss : 2.301933\n",
      "113 iters  acc : 0.171875 loss : 2.3012466\n",
      "114 iters  acc : 0.1640625 loss : 2.3009548\n",
      "115 iters  acc : 0.1484375 loss : 2.3005958\n",
      "116 iters  acc : 0.1484375 loss : 2.3008285\n",
      "117 iters  acc : 0.21484375 loss : 2.298655\n",
      "118 iters  acc : 0.1015625 loss : 2.3001633\n",
      "119 iters  acc : 0.09375 loss : 2.3011942\n",
      "120 iters  acc : 0.125 loss : 2.2993333\n",
      "121 iters  acc : 0.1484375 loss : 2.2979867\n",
      "122 iters  acc : 0.12109375 loss : 2.3011541\n",
      "123 iters  acc : 0.13671875 loss : 2.301107\n",
      "124 iters  acc : 0.1796875 loss : 2.2971053\n",
      "125 iters  acc : 0.10546875 loss : 2.299883\n",
      "126 iters  acc : 0.1875 loss : 2.2974057\n",
      "127 iters  acc : 0.08984375 loss : 2.3000011\n",
      "128 iters  acc : 0.1328125 loss : 2.2967062\n",
      "129 iters  acc : 0.17578125 loss : 2.29674\n",
      "130 iters  acc : 0.12109375 loss : 2.3019202\n",
      "131 iters  acc : 0.109375 loss : 2.2983632\n",
      "132 iters  acc : 0.09765625 loss : 2.2983165\n",
      "133 iters  acc : 0.10546875 loss : 2.2983284\n",
      "134 iters  acc : 0.1015625 loss : 2.3039336\n",
      "135 iters  acc : 0.1328125 loss : 2.2973661\n",
      "136 iters  acc : 0.15234375 loss : 2.294584\n",
      "137 iters  acc : 0.1328125 loss : 2.2981205\n",
      "138 iters  acc : 0.1015625 loss : 2.297769\n",
      "139 iters  acc : 0.125 loss : 2.3000894\n",
      "140 iters  acc : 0.19921875 loss : 2.292965\n",
      "141 iters  acc : 0.07421875 loss : 2.3044834\n",
      "142 iters  acc : 0.0859375 loss : 2.3003702\n",
      "143 iters  acc : 0.1640625 loss : 2.2909715\n",
      "144 iters  acc : 0.12109375 loss : 2.2982535\n",
      "145 iters  acc : 0.12109375 loss : 2.2915413\n",
      "146 iters  acc : 0.18359375 loss : 2.291575\n",
      "147 iters  acc : 0.15625 loss : 2.301065\n",
      "148 iters  acc : 0.09375 loss : 2.2997181\n",
      "149 iters  acc : 0.15234375 loss : 2.2918048\n",
      "150 iters  acc : 0.08984375 loss : 2.296166\n",
      "151 iters  acc : 0.125 loss : 2.298675\n",
      "152 iters  acc : 0.1171875 loss : 2.2910728\n",
      "153 iters  acc : 0.15234375 loss : 2.2837148\n",
      "154 iters  acc : 0.18359375 loss : 2.2633328\n",
      "155 iters  acc : 0.08984375 loss : 2.320661\n",
      "156 iters  acc : 0.109375 loss : 2.3084245\n",
      "157 iters  acc : 0.1171875 loss : 2.303031\n",
      "158 iters  acc : 0.1015625 loss : 2.3035264\n",
      "159 iters  acc : 0.07421875 loss : 2.3037922\n",
      "160 iters  acc : 0.1171875 loss : 2.2984257\n",
      "161 iters  acc : 0.140625 loss : 2.2971754\n",
      "162 iters  acc : 0.140625 loss : 2.2869134\n",
      "163 iters  acc : 0.12109375 loss : 2.282304\n",
      "164 iters  acc : 0.14453125 loss : 2.2826753\n",
      "165 iters  acc : 0.12109375 loss : 2.2873487\n",
      "166 iters  acc : 0.1875 loss : 2.2706795\n",
      "167 iters  acc : 0.1328125 loss : 2.2775283\n",
      "168 iters  acc : 0.125 loss : 2.3096168\n",
      "169 iters  acc : 0.14453125 loss : 2.2885966\n",
      "170 iters  acc : 0.12109375 loss : 2.291621\n",
      "171 iters  acc : 0.17578125 loss : 2.2729115\n",
      "172 iters  acc : 0.14453125 loss : 2.2731023\n",
      "173 iters  acc : 0.2109375 loss : 2.2620707\n",
      "174 iters  acc : 0.10546875 loss : 2.3165631\n",
      "175 iters  acc : 0.24609375 loss : 2.2710466\n",
      "176 iters  acc : 0.1796875 loss : 2.2687616\n",
      "177 iters  acc : 0.171875 loss : 2.2539542\n",
      "178 iters  acc : 0.17578125 loss : 2.2428176\n",
      "179 iters  acc : 0.17578125 loss : 2.2415211\n",
      "180 iters  acc : 0.11328125 loss : 2.3314023\n",
      "181 iters  acc : 0.14453125 loss : 2.2716784\n",
      "182 iters  acc : 0.23828125 loss : 2.2463956\n",
      "183 iters  acc : 0.1953125 loss : 2.2253213\n",
      "184 iters  acc : 0.14453125 loss : 2.298057\n",
      "185 iters  acc : 0.10546875 loss : 2.3196638\n",
      "186 iters  acc : 0.125 loss : 2.305008\n",
      "187 iters  acc : 0.19140625 loss : 2.273086\n",
      "188 iters  acc : 0.1796875 loss : 2.2312489\n",
      "189 iters  acc : 0.1640625 loss : 2.2546525\n",
      "190 iters  acc : 0.2421875 loss : 2.2405553\n",
      "191 iters  acc : 0.19140625 loss : 2.2525783\n",
      "192 iters  acc : 0.1640625 loss : 2.2475665\n",
      "193 iters  acc : 0.15625 loss : 2.2514186\n",
      "194 iters  acc : 0.23046875 loss : 2.239386\n",
      "195 iters  acc : 0.234375 loss : 2.1947527\n",
      "196 iters  acc : 0.234375 loss : 2.1742463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 iters  acc : 0.1328125 loss : 2.492179\n",
      "198 iters  acc : 0.15625 loss : 2.2791636\n",
      "199 iters  acc : 0.2265625 loss : 2.2038128\n",
      "200 iters  acc : 0.234375 loss : 2.1870766\n",
      "201 iters  acc : 0.16015625 loss : 2.2378798\n",
      "202 iters  acc : 0.21484375 loss : 2.2263503\n",
      "203 iters  acc : 0.18359375 loss : 2.264629\n",
      "204 iters  acc : 0.125 loss : 2.2824163\n",
      "205 iters  acc : 0.1328125 loss : 2.265715\n",
      "206 iters  acc : 0.25 loss : 2.2368534\n",
      "207 iters  acc : 0.26171875 loss : 2.1835036\n",
      "208 iters  acc : 0.265625 loss : 2.1679943\n",
      "209 iters  acc : 0.23828125 loss : 2.2103822\n",
      "210 iters  acc : 0.234375 loss : 2.1763854\n",
      "211 iters  acc : 0.19140625 loss : 2.2651768\n",
      "212 iters  acc : 0.13671875 loss : 2.3019395\n",
      "213 iters  acc : 0.1015625 loss : 2.3098993\n",
      "214 iters  acc : 0.20703125 loss : 2.2330563\n",
      "215 iters  acc : 0.19921875 loss : 2.2194881\n",
      "216 iters  acc : 0.203125 loss : 2.2224362\n",
      "217 iters  acc : 0.2578125 loss : 2.1699514\n",
      "218 iters  acc : 0.25390625 loss : 2.1762419\n",
      "219 iters  acc : 0.1953125 loss : 2.231815\n",
      "220 iters  acc : 0.1875 loss : 2.227325\n",
      "221 iters  acc : 0.22265625 loss : 2.1686907\n",
      "222 iters  acc : 0.1953125 loss : 2.2189136\n",
      "223 iters  acc : 0.23828125 loss : 2.209069\n",
      "224 iters  acc : 0.17578125 loss : 2.2556171\n",
      "225 iters  acc : 0.171875 loss : 2.261313\n",
      "226 iters  acc : 0.21875 loss : 2.2017002\n",
      "227 iters  acc : 0.21484375 loss : 2.2191978\n",
      "228 iters  acc : 0.22265625 loss : 2.200261\n",
      "229 iters  acc : 0.22265625 loss : 2.1830406\n",
      "230 iters  acc : 0.234375 loss : 2.181623\n",
      "231 iters  acc : 0.15234375 loss : 2.2506075\n",
      "232 iters  acc : 0.140625 loss : 2.2911813\n",
      "233 iters  acc : 0.2109375 loss : 2.234529\n",
      "234 iters  acc : 0.2734375 loss : 2.1889565\n",
      "235 iters  acc : 0.26171875 loss : 2.1542304\n",
      "236 iters  acc : 0.26171875 loss : 2.140805\n",
      "237 iters  acc : 0.2421875 loss : 2.1998813\n",
      "238 iters  acc : 0.19921875 loss : 2.1991363\n",
      "239 iters  acc : 0.21484375 loss : 2.2003303\n",
      "240 iters  acc : 0.21484375 loss : 2.1862845\n",
      "241 iters  acc : 0.234375 loss : 2.187644\n",
      "242 iters  acc : 0.19921875 loss : 2.2165775\n",
      "243 iters  acc : 0.23828125 loss : 2.1717434\n",
      "244 iters  acc : 0.234375 loss : 2.117848\n",
      "245 iters  acc : 0.22265625 loss : 2.2278714\n",
      "246 iters  acc : 0.26171875 loss : 2.174945\n",
      "247 iters  acc : 0.2734375 loss : 2.123734\n",
      "248 iters  acc : 0.2265625 loss : 2.17692\n",
      "249 iters  acc : 0.23828125 loss : 2.1908953\n",
      "250 iters  acc : 0.203125 loss : 2.1954765\n",
      "251 iters  acc : 0.1953125 loss : 2.2655263\n",
      "252 iters  acc : 0.19921875 loss : 2.2179763\n",
      "253 iters  acc : 0.265625 loss : 2.1587088\n",
      "254 iters  acc : 0.22265625 loss : 2.1995025\n",
      "255 iters  acc : 0.23046875 loss : 2.198909\n",
      "256 iters  acc : 0.234375 loss : 2.1917112\n",
      "257 iters  acc : 0.2578125 loss : 2.1508474\n",
      "258 iters  acc : 0.2578125 loss : 2.1227527\n",
      "259 iters  acc : 0.21875 loss : 2.2484922\n",
      "260 iters  acc : 0.28125 loss : 2.143064\n",
      "261 iters  acc : 0.1953125 loss : 2.2663357\n",
      "262 iters  acc : 0.171875 loss : 2.2663028\n",
      "263 iters  acc : 0.15234375 loss : 2.261726\n",
      "264 iters  acc : 0.328125 loss : 2.0937243\n",
      "265 iters  acc : 0.25 loss : 2.1576715\n",
      "266 iters  acc : 0.2578125 loss : 2.145447\n",
      "267 iters  acc : 0.265625 loss : 2.1694741\n",
      "268 iters  acc : 0.2421875 loss : 2.2042778\n",
      "269 iters  acc : 0.25390625 loss : 2.122664\n",
      "270 iters  acc : 0.28515625 loss : 2.1266303\n",
      "271 iters  acc : 0.25390625 loss : 2.152111\n",
      "272 iters  acc : 0.296875 loss : 2.122861\n",
      "273 iters  acc : 0.26953125 loss : 2.128201\n",
      "274 iters  acc : 0.23828125 loss : 2.1578674\n",
      "275 iters  acc : 0.2109375 loss : 2.1895838\n",
      "276 iters  acc : 0.30859375 loss : 2.1072083\n",
      "277 iters  acc : 0.2890625 loss : 2.0926943\n",
      "278 iters  acc : 0.3046875 loss : 2.0932615\n",
      "279 iters  acc : 0.203125 loss : 2.2279139\n",
      "280 iters  acc : 0.12890625 loss : 2.4503617\n",
      "281 iters  acc : 0.16015625 loss : 2.2416534\n",
      "282 iters  acc : 0.2421875 loss : 2.171579\n",
      "283 iters  acc : 0.265625 loss : 2.1477077\n",
      "284 iters  acc : 0.22265625 loss : 2.1840374\n",
      "285 iters  acc : 0.265625 loss : 2.1562834\n",
      "286 iters  acc : 0.23828125 loss : 2.1777117\n",
      "287 iters  acc : 0.2890625 loss : 2.1219752\n",
      "288 iters  acc : 0.26171875 loss : 2.1150484\n",
      "289 iters  acc : 0.234375 loss : 2.1706905\n",
      "290 iters  acc : 0.2265625 loss : 2.1943166\n",
      "291 iters  acc : 0.2734375 loss : 2.1341233\n",
      "292 iters  acc : 0.23046875 loss : 2.167037\n",
      "293 iters  acc : 0.25390625 loss : 2.1933112\n",
      "294 iters  acc : 0.22265625 loss : 2.1780715\n",
      "295 iters  acc : 0.2734375 loss : 2.1255145\n",
      "296 iters  acc : 0.20703125 loss : 2.2435846\n",
      "297 iters  acc : 0.21875 loss : 2.1781242\n",
      "298 iters  acc : 0.21484375 loss : 2.1972146\n",
      "299 iters  acc : 0.27734375 loss : 2.1425138\n",
      "300 iters  acc : 0.26171875 loss : 2.1030586\n",
      "301 iters  acc : 0.2734375 loss : 2.1462002\n",
      "302 iters  acc : 0.23046875 loss : 2.204143\n",
      "303 iters  acc : 0.21875 loss : 2.1909196\n",
      "304 iters  acc : 0.21484375 loss : 2.183228\n",
      "305 iters  acc : 0.25390625 loss : 2.1469285\n",
      "306 iters  acc : 0.26953125 loss : 2.119211\n",
      "307 iters  acc : 0.2734375 loss : 2.1377118\n",
      "308 iters  acc : 0.234375 loss : 2.1633172\n",
      "309 iters  acc : 0.2109375 loss : 2.1762166\n",
      "310 iters  acc : 0.24609375 loss : 2.1678061\n",
      "311 iters  acc : 0.328125 loss : 2.0651166\n",
      "312 iters  acc : 0.26953125 loss : 2.18734\n",
      "313 iters  acc : 0.1875 loss : 2.2386568\n",
      "314 iters  acc : 0.24609375 loss : 2.164795\n",
      "315 iters  acc : 0.26171875 loss : 2.1432893\n",
      "316 iters  acc : 0.3046875 loss : 2.104289\n",
      "317 iters  acc : 0.23828125 loss : 2.1368713\n",
      "318 iters  acc : 0.2578125 loss : 2.1391792\n",
      "319 iters  acc : 0.23046875 loss : 2.1973722\n",
      "320 iters  acc : 0.2734375 loss : 2.1229825\n",
      "321 iters  acc : 0.265625 loss : 2.1173053\n",
      "322 iters  acc : 0.2421875 loss : 2.1437619\n",
      "323 iters  acc : 0.27734375 loss : 2.1156988\n",
      "324 iters  acc : 0.16796875 loss : 2.276202\n",
      "325 iters  acc : 0.234375 loss : 2.1351173\n",
      "326 iters  acc : 0.28125 loss : 2.1239395\n",
      "327 iters  acc : 0.25390625 loss : 2.1529117\n",
      "328 iters  acc : 0.29296875 loss : 2.0994987\n",
      "329 iters  acc : 0.25 loss : 2.1412408\n",
      "330 iters  acc : 0.2734375 loss : 2.1524699\n",
      "331 iters  acc : 0.21875 loss : 2.144487\n",
      "332 iters  acc : 0.2578125 loss : 2.1071568\n",
      "333 iters  acc : 0.19140625 loss : 2.229291\n",
      "334 iters  acc : 0.25390625 loss : 2.1206563\n",
      "335 iters  acc : 0.22265625 loss : 2.1758013\n",
      "336 iters  acc : 0.265625 loss : 2.136322\n",
      "337 iters  acc : 0.25 loss : 2.104519\n",
      "338 iters  acc : 0.23828125 loss : 2.135776\n",
      "339 iters  acc : 0.27734375 loss : 2.1260343\n",
      "340 iters  acc : 0.25390625 loss : 2.1349874\n",
      "341 iters  acc : 0.17578125 loss : 2.291547\n",
      "342 iters  acc : 0.25390625 loss : 2.158361\n",
      "343 iters  acc : 0.25390625 loss : 2.1484005\n",
      "344 iters  acc : 0.20703125 loss : 2.187078\n",
      "345 iters  acc : 0.2109375 loss : 2.1880066\n",
      "346 iters  acc : 0.32421875 loss : 2.0900605\n",
      "347 iters  acc : 0.30078125 loss : 2.0497146\n",
      "348 iters  acc : 0.234375 loss : 2.1438825\n",
      "349 iters  acc : 0.21875 loss : 2.2758741\n",
      "350 iters  acc : 0.13671875 loss : 2.35188\n",
      "351 iters  acc : 0.25 loss : 2.1894073\n",
      "352 iters  acc : 0.25 loss : 2.1523352\n",
      "353 iters  acc : 0.26953125 loss : 2.0908554\n",
      "354 iters  acc : 0.3359375 loss : 2.0654182\n",
      "355 iters  acc : 0.26171875 loss : 2.1542063\n",
      "356 iters  acc : 0.2265625 loss : 2.1266582\n",
      "357 iters  acc : 0.24609375 loss : 2.1419296\n",
      "358 iters  acc : 0.23828125 loss : 2.115894\n",
      "359 iters  acc : 0.25390625 loss : 2.1699324\n",
      "360 iters  acc : 0.234375 loss : 2.19772\n",
      "361 iters  acc : 0.23828125 loss : 2.1684704\n",
      "362 iters  acc : 0.2734375 loss : 2.1103544\n",
      "363 iters  acc : 0.2734375 loss : 2.1392016\n",
      "364 iters  acc : 0.30078125 loss : 2.110382\n",
      "365 iters  acc : 0.27734375 loss : 2.1139297\n",
      "366 iters  acc : 0.27734375 loss : 2.131216\n",
      "367 iters  acc : 0.23828125 loss : 2.174978\n",
      "368 iters  acc : 0.27734375 loss : 2.1577554\n",
      "369 iters  acc : 0.24609375 loss : 2.1437054\n",
      "370 iters  acc : 0.2890625 loss : 2.1400692\n",
      "371 iters  acc : 0.296875 loss : 2.0874813\n",
      "372 iters  acc : 0.24609375 loss : 2.1267989\n",
      "373 iters  acc : 0.28125 loss : 2.0905867\n",
      "374 iters  acc : 0.296875 loss : 2.1035042\n",
      "375 iters  acc : 0.265625 loss : 2.153206\n",
      "376 iters  acc : 0.2578125 loss : 2.119478\n",
      "377 iters  acc : 0.26953125 loss : 2.132834\n",
      "378 iters  acc : 0.25390625 loss : 2.076111\n",
      "379 iters  acc : 0.2734375 loss : 2.0475264\n",
      "380 iters  acc : 0.24609375 loss : 2.174474\n",
      "381 iters  acc : 0.34765625 loss : 2.0300198\n",
      "382 iters  acc : 0.265625 loss : 2.130805\n",
      "383 iters  acc : 0.24609375 loss : 2.1707997\n",
      "384 iters  acc : 0.27734375 loss : 2.131554\n",
      "385 iters  acc : 0.26171875 loss : 2.1019237\n",
      "386 iters  acc : 0.3046875 loss : 2.084474\n",
      "387 iters  acc : 0.32421875 loss : 2.0283241\n",
      "388 iters  acc : 0.296875 loss : 2.0451016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389 iters  acc : 0.28125 loss : 2.0760608\n",
      "390 iters  acc : 0.3203125 loss : 2.0055685\n",
      "391 iters  acc : 0.21484375 loss : 2.2219296\n",
      "392 iters  acc : 0.17578125 loss : 2.4059014\n",
      "393 iters  acc : 0.24609375 loss : 2.1489778\n",
      "394 iters  acc : 0.234375 loss : 2.1520777\n",
      "395 iters  acc : 0.296875 loss : 2.0990024\n",
      "396 iters  acc : 0.23046875 loss : 2.16818\n",
      "397 iters  acc : 0.34765625 loss : 2.0641468\n",
      "398 iters  acc : 0.265625 loss : 2.1293213\n",
      "399 iters  acc : 0.23828125 loss : 2.1127853\n",
      "400 iters  acc : 0.28125 loss : 2.082206\n",
      "401 iters  acc : 0.21875 loss : 2.1565542\n",
      "402 iters  acc : 0.3125 loss : 2.026511\n",
      "403 iters  acc : 0.30859375 loss : 2.0743313\n",
      "404 iters  acc : 0.296875 loss : 2.0611053\n",
      "405 iters  acc : 0.3203125 loss : 2.0399303\n",
      "406 iters  acc : 0.3125 loss : 2.0201573\n",
      "407 iters  acc : 0.31640625 loss : 2.0856428\n",
      "408 iters  acc : 0.24609375 loss : 2.2098622\n",
      "409 iters  acc : 0.27734375 loss : 2.1205268\n",
      "410 iters  acc : 0.30078125 loss : 2.0728877\n",
      "411 iters  acc : 0.36328125 loss : 2.0163932\n",
      "412 iters  acc : 0.27734375 loss : 2.1069767\n",
      "413 iters  acc : 0.33203125 loss : 1.99334\n",
      "414 iters  acc : 0.32421875 loss : 2.029584\n",
      "415 iters  acc : 0.24609375 loss : 2.1757865\n",
      "416 iters  acc : 0.24609375 loss : 2.1272278\n",
      "417 iters  acc : 0.28125 loss : 2.0643942\n",
      "418 iters  acc : 0.2890625 loss : 2.0811932\n",
      "419 iters  acc : 0.32421875 loss : 2.0338092\n",
      "420 iters  acc : 0.23828125 loss : 2.1700401\n",
      "421 iters  acc : 0.25390625 loss : 2.1187353\n",
      "422 iters  acc : 0.28125 loss : 2.1030688\n",
      "423 iters  acc : 0.2578125 loss : 2.1475425\n",
      "424 iters  acc : 0.3203125 loss : 2.0253434\n",
      "425 iters  acc : 0.265625 loss : 2.1474862\n",
      "426 iters  acc : 0.27734375 loss : 2.0957541\n",
      "427 iters  acc : 0.26171875 loss : 2.1380997\n",
      "428 iters  acc : 0.265625 loss : 2.0650537\n",
      "429 iters  acc : 0.2109375 loss : 2.17079\n",
      "430 iters  acc : 0.29296875 loss : 2.0244677\n",
      "431 iters  acc : 0.22265625 loss : 2.157491\n",
      "432 iters  acc : 0.23046875 loss : 2.1392977\n",
      "433 iters  acc : 0.3203125 loss : 2.0659585\n",
      "434 iters  acc : 0.26171875 loss : 2.0931733\n",
      "435 iters  acc : 0.3203125 loss : 2.1173656\n",
      "436 iters  acc : 0.27734375 loss : 2.168775\n",
      "437 iters  acc : 0.1953125 loss : 2.2043023\n",
      "438 iters  acc : 0.3515625 loss : 2.0413108\n",
      "439 iters  acc : 0.2890625 loss : 2.0597887\n",
      "440 iters  acc : 0.29296875 loss : 2.124043\n",
      "441 iters  acc : 0.3046875 loss : 2.0337217\n",
      "442 iters  acc : 0.30078125 loss : 2.119116\n",
      "443 iters  acc : 0.2578125 loss : 2.1238656\n",
      "444 iters  acc : 0.27734375 loss : 2.113999\n",
      "445 iters  acc : 0.25 loss : 2.1776748\n",
      "446 iters  acc : 0.3203125 loss : 2.040779\n",
      "447 iters  acc : 0.3125 loss : 2.042038\n",
      "448 iters  acc : 0.2734375 loss : 2.1099184\n",
      "449 iters  acc : 0.26171875 loss : 2.0639884\n",
      "450 iters  acc : 0.25390625 loss : 2.1661327\n",
      "451 iters  acc : 0.25390625 loss : 2.1435108\n",
      "452 iters  acc : 0.28125 loss : 2.068047\n",
      "453 iters  acc : 0.2265625 loss : 2.1804864\n",
      "454 iters  acc : 0.29296875 loss : 2.1152582\n",
      "455 iters  acc : 0.26953125 loss : 2.1336045\n",
      "456 iters  acc : 0.31640625 loss : 2.0639138\n",
      "457 iters  acc : 0.33984375 loss : 2.0069435\n",
      "458 iters  acc : 0.28515625 loss : 2.1702926\n",
      "459 iters  acc : 0.24609375 loss : 2.1282096\n",
      "460 iters  acc : 0.27734375 loss : 2.1232276\n",
      "461 iters  acc : 0.26953125 loss : 2.1203494\n",
      "462 iters  acc : 0.265625 loss : 2.0855699\n",
      "463 iters  acc : 0.3359375 loss : 2.0327916\n",
      "464 iters  acc : 0.296875 loss : 2.0636806\n",
      "465 iters  acc : 0.31640625 loss : 2.0023851\n",
      "466 iters  acc : 0.27734375 loss : 2.139896\n",
      "467 iters  acc : 0.29296875 loss : 2.051744\n",
      "468 iters  acc : 0.29296875 loss : 2.0528688\n",
      "469 iters  acc : 0.32421875 loss : 2.0160427\n",
      "470 iters  acc : 0.296875 loss : 2.0897014\n",
      "471 iters  acc : 0.26953125 loss : 2.1274078\n",
      "472 iters  acc : 0.296875 loss : 2.0997608\n",
      "473 iters  acc : 0.3046875 loss : 2.0831556\n",
      "474 iters  acc : 0.25390625 loss : 2.1442888\n",
      "475 iters  acc : 0.33984375 loss : 2.0310838\n",
      "476 iters  acc : 0.3046875 loss : 2.0042884\n",
      "477 iters  acc : 0.22265625 loss : 2.182223\n",
      "478 iters  acc : 0.2890625 loss : 2.0655034\n",
      "479 iters  acc : 0.34375 loss : 2.0075347\n",
      "480 iters  acc : 0.24609375 loss : 2.1540935\n",
      "481 iters  acc : 0.26953125 loss : 2.0997543\n",
      "482 iters  acc : 0.2890625 loss : 2.0893667\n",
      "483 iters  acc : 0.3046875 loss : 2.071676\n",
      "484 iters  acc : 0.3125 loss : 2.0706773\n",
      "485 iters  acc : 0.27734375 loss : 2.0771134\n",
      "486 iters  acc : 0.29296875 loss : 2.0734105\n",
      "487 iters  acc : 0.25 loss : 2.1083093\n",
      "488 iters  acc : 0.2734375 loss : 2.093028\n",
      "489 iters  acc : 0.26953125 loss : 2.1313417\n",
      "490 iters  acc : 0.26171875 loss : 2.118426\n",
      "491 iters  acc : 0.30078125 loss : 2.09176\n",
      "492 iters  acc : 0.30859375 loss : 2.0567765\n",
      "493 iters  acc : 0.26953125 loss : 2.0319376\n",
      "494 iters  acc : 0.2109375 loss : 2.2133439\n",
      "495 iters  acc : 0.234375 loss : 2.214818\n",
      "496 iters  acc : 0.29296875 loss : 2.0736108\n",
      "497 iters  acc : 0.265625 loss : 2.099579\n",
      "498 iters  acc : 0.3359375 loss : 2.0121999\n",
      "499 iters  acc : 0.3359375 loss : 1.9738042\n",
      "500 iters  acc : 0.3125 loss : 2.0036383\n",
      "501 iters  acc : 0.23828125 loss : 2.127133\n",
      "502 iters  acc : 0.24609375 loss : 2.1656342\n",
      "503 iters  acc : 0.34765625 loss : 2.0308104\n",
      "504 iters  acc : 0.37890625 loss : 2.009449\n",
      "505 iters  acc : 0.34375 loss : 2.069832\n",
      "506 iters  acc : 0.2265625 loss : 2.1793501\n",
      "507 iters  acc : 0.234375 loss : 2.150979\n",
      "508 iters  acc : 0.3125 loss : 2.0779681\n",
      "509 iters  acc : 0.34765625 loss : 2.0101385\n",
      "510 iters  acc : 0.3203125 loss : 1.9914348\n",
      "511 iters  acc : 0.27734375 loss : 2.0595384\n",
      "512 iters  acc : 0.29296875 loss : 2.0975935\n",
      "513 iters  acc : 0.3203125 loss : 2.0607378\n",
      "514 iters  acc : 0.30859375 loss : 1.9987079\n",
      "515 iters  acc : 0.26953125 loss : 2.0730581\n",
      "516 iters  acc : 0.30859375 loss : 2.0422163\n",
      "517 iters  acc : 0.33984375 loss : 1.9354696\n",
      "518 iters  acc : 0.28515625 loss : 2.1031408\n",
      "519 iters  acc : 0.3046875 loss : 2.0966802\n",
      "520 iters  acc : 0.265625 loss : 2.141941\n",
      "521 iters  acc : 0.21875 loss : 2.2067146\n",
      "522 iters  acc : 0.31640625 loss : 2.0312254\n",
      "523 iters  acc : 0.31640625 loss : 2.0207562\n",
      "524 iters  acc : 0.31640625 loss : 2.0511317\n",
      "525 iters  acc : 0.3125 loss : 2.0490506\n",
      "526 iters  acc : 0.2734375 loss : 2.087915\n",
      "527 iters  acc : 0.29296875 loss : 2.0540905\n",
      "528 iters  acc : 0.29296875 loss : 2.046744\n",
      "529 iters  acc : 0.28125 loss : 2.085249\n",
      "530 iters  acc : 0.27734375 loss : 2.0900936\n",
      "531 iters  acc : 0.34765625 loss : 1.9879389\n",
      "532 iters  acc : 0.3828125 loss : 1.8975552\n",
      "533 iters  acc : 0.26171875 loss : 2.1782112\n",
      "534 iters  acc : 0.19921875 loss : 2.3155944\n",
      "535 iters  acc : 0.27734375 loss : 2.1478877\n",
      "536 iters  acc : 0.25390625 loss : 2.1038032\n",
      "537 iters  acc : 0.3125 loss : 2.0423205\n",
      "538 iters  acc : 0.3125 loss : 2.033678\n",
      "539 iters  acc : 0.296875 loss : 2.0525117\n",
      "540 iters  acc : 0.31640625 loss : 1.9818765\n",
      "541 iters  acc : 0.2734375 loss : 2.062152\n",
      "542 iters  acc : 0.28125 loss : 2.078366\n",
      "543 iters  acc : 0.33203125 loss : 2.0079832\n",
      "544 iters  acc : 0.31640625 loss : 2.0080485\n",
      "545 iters  acc : 0.3359375 loss : 2.0271697\n",
      "546 iters  acc : 0.33203125 loss : 1.9890633\n",
      "547 iters  acc : 0.23046875 loss : 2.282165\n",
      "548 iters  acc : 0.265625 loss : 2.1068263\n",
      "549 iters  acc : 0.296875 loss : 2.0717287\n",
      "550 iters  acc : 0.26171875 loss : 2.114333\n",
      "551 iters  acc : 0.30859375 loss : 2.0306673\n",
      "552 iters  acc : 0.2734375 loss : 2.1069539\n",
      "553 iters  acc : 0.33984375 loss : 2.0119653\n",
      "554 iters  acc : 0.25390625 loss : 2.0988822\n",
      "555 iters  acc : 0.30078125 loss : 2.0276082\n",
      "556 iters  acc : 0.296875 loss : 2.0232248\n",
      "557 iters  acc : 0.234375 loss : 2.1610036\n",
      "558 iters  acc : 0.265625 loss : 2.1187384\n",
      "559 iters  acc : 0.33984375 loss : 1.9553324\n",
      "560 iters  acc : 0.296875 loss : 2.0632243\n",
      "561 iters  acc : 0.28515625 loss : 2.083859\n",
      "562 iters  acc : 0.27734375 loss : 2.136444\n",
      "563 iters  acc : 0.31640625 loss : 2.0394444\n",
      "564 iters  acc : 0.3125 loss : 2.0407128\n",
      "565 iters  acc : 0.27734375 loss : 2.1539779\n",
      "566 iters  acc : 0.21875 loss : 2.2097285\n",
      "567 iters  acc : 0.28125 loss : 2.1003132\n",
      "568 iters  acc : 0.25 loss : 2.0943127\n",
      "569 iters  acc : 0.32421875 loss : 2.0122075\n",
      "570 iters  acc : 0.3359375 loss : 1.9731772\n",
      "571 iters  acc : 0.30078125 loss : 2.0690622\n",
      "572 iters  acc : 0.30859375 loss : 2.0663905\n",
      "573 iters  acc : 0.3125 loss : 2.0194082\n",
      "574 iters  acc : 0.32421875 loss : 2.0055127\n",
      "575 iters  acc : 0.3203125 loss : 1.961704\n",
      "576 iters  acc : 0.3046875 loss : 2.009779\n",
      "577 iters  acc : 0.27734375 loss : 2.1007018\n",
      "578 iters  acc : 0.30859375 loss : 2.064661\n",
      "579 iters  acc : 0.28515625 loss : 2.1109638\n",
      "580 iters  acc : 0.3125 loss : 2.0121589\n",
      "581 iters  acc : 0.3046875 loss : 1.9911315\n",
      "582 iters  acc : 0.32421875 loss : 2.0410335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 iters  acc : 0.33984375 loss : 2.0613341\n",
      "584 iters  acc : 0.296875 loss : 2.1010993\n",
      "585 iters  acc : 0.30859375 loss : 2.0360472\n",
      "586 iters  acc : 0.328125 loss : 1.9751613\n",
      "587 iters  acc : 0.30859375 loss : 2.041833\n",
      "588 iters  acc : 0.34765625 loss : 2.011273\n",
      "589 iters  acc : 0.29296875 loss : 2.0664353\n",
      "590 iters  acc : 0.27734375 loss : 2.0296435\n",
      "591 iters  acc : 0.3125 loss : 2.0451293\n",
      "592 iters  acc : 0.30078125 loss : 2.0005455\n",
      "593 iters  acc : 0.3203125 loss : 2.003147\n",
      "594 iters  acc : 0.30859375 loss : 2.0126624\n",
      "595 iters  acc : 0.28125 loss : 2.094492\n",
      "596 iters  acc : 0.25 loss : 2.125669\n",
      "597 iters  acc : 0.3203125 loss : 2.0201108\n",
      "598 iters  acc : 0.30859375 loss : 1.976367\n",
      "599 iters  acc : 0.265625 loss : 2.1951802\n",
      "600 iters  acc : 0.25 loss : 2.1305928\n",
      "601 iters  acc : 0.30859375 loss : 2.044801\n",
      "602 iters  acc : 0.3359375 loss : 1.984576\n",
      "603 iters  acc : 0.27734375 loss : 2.1124792\n",
      "604 iters  acc : 0.2578125 loss : 2.0541363\n",
      "605 iters  acc : 0.30859375 loss : 2.0003538\n",
      "606 iters  acc : 0.359375 loss : 1.9010608\n",
      "607 iters  acc : 0.2734375 loss : 2.1740446\n",
      "608 iters  acc : 0.25390625 loss : 2.1675344\n",
      "609 iters  acc : 0.234375 loss : 2.1696608\n",
      "610 iters  acc : 0.28515625 loss : 2.0592208\n",
      "611 iters  acc : 0.26171875 loss : 2.0722528\n",
      "612 iters  acc : 0.33984375 loss : 1.965313\n",
      "613 iters  acc : 0.296875 loss : 2.0560153\n",
      "614 iters  acc : 0.30078125 loss : 2.0342078\n",
      "615 iters  acc : 0.28125 loss : 2.0329843\n",
      "616 iters  acc : 0.28125 loss : 2.0745323\n",
      "617 iters  acc : 0.2890625 loss : 2.0595298\n",
      "618 iters  acc : 0.3046875 loss : 2.0054908\n",
      "619 iters  acc : 0.28125 loss : 2.0440588\n",
      "620 iters  acc : 0.3203125 loss : 2.0313053\n",
      "621 iters  acc : 0.3125 loss : 2.0503936\n",
      "622 iters  acc : 0.28515625 loss : 2.0545533\n",
      "623 iters  acc : 0.33984375 loss : 1.9766302\n",
      "624 iters  acc : 0.34375 loss : 1.9561241\n",
      "625 iters  acc : 0.3359375 loss : 1.9820938\n",
      "626 iters  acc : 0.34765625 loss : 1.9761846\n",
      "627 iters  acc : 0.28125 loss : 2.1011083\n",
      "628 iters  acc : 0.33984375 loss : 2.025186\n",
      "629 iters  acc : 0.3125 loss : 2.0661151\n",
      "630 iters  acc : 0.2890625 loss : 2.1215656\n",
      "631 iters  acc : 0.3359375 loss : 2.0480032\n",
      "632 iters  acc : 0.3359375 loss : 2.0028186\n",
      "633 iters  acc : 0.375 loss : 1.9501847\n",
      "634 iters  acc : 0.30078125 loss : 2.054534\n",
      "635 iters  acc : 0.3203125 loss : 2.003596\n",
      "636 iters  acc : 0.33984375 loss : 1.9770536\n",
      "637 iters  acc : 0.328125 loss : 2.0409927\n",
      "638 iters  acc : 0.3515625 loss : 1.972303\n",
      "639 iters  acc : 0.30859375 loss : 2.0355034\n",
      "640 iters  acc : 0.29296875 loss : 2.0746975\n",
      "641 iters  acc : 0.34765625 loss : 2.012706\n",
      "642 iters  acc : 0.265625 loss : 2.1483786\n",
      "643 iters  acc : 0.28125 loss : 2.1123471\n",
      "644 iters  acc : 0.3203125 loss : 2.0188541\n",
      "645 iters  acc : 0.296875 loss : 2.0208569\n",
      "646 iters  acc : 0.296875 loss : 2.0288422\n",
      "647 iters  acc : 0.296875 loss : 2.0256004\n",
      "648 iters  acc : 0.3046875 loss : 2.0675213\n",
      "649 iters  acc : 0.27734375 loss : 2.0270302\n",
      "650 iters  acc : 0.3671875 loss : 1.9209352\n",
      "651 iters  acc : 0.32421875 loss : 1.9158838\n",
      "652 iters  acc : 0.3125 loss : 2.0427892\n",
      "653 iters  acc : 0.28125 loss : 2.0290995\n",
      "654 iters  acc : 0.2578125 loss : 2.150793\n",
      "655 iters  acc : 0.3046875 loss : 2.0403106\n",
      "656 iters  acc : 0.3359375 loss : 1.967868\n",
      "657 iters  acc : 0.296875 loss : 2.0755448\n",
      "658 iters  acc : 0.3125 loss : 2.024363\n",
      "659 iters  acc : 0.28515625 loss : 2.0649137\n",
      "660 iters  acc : 0.32421875 loss : 1.9577079\n",
      "661 iters  acc : 0.28125 loss : 2.0932894\n",
      "662 iters  acc : 0.29296875 loss : 2.1493156\n",
      "663 iters  acc : 0.35546875 loss : 1.9785869\n",
      "664 iters  acc : 0.32421875 loss : 2.0213752\n",
      "665 iters  acc : 0.25390625 loss : 2.1332679\n",
      "666 iters  acc : 0.359375 loss : 1.9088036\n",
      "667 iters  acc : 0.31640625 loss : 2.044106\n",
      "668 iters  acc : 0.27734375 loss : 2.0460916\n",
      "669 iters  acc : 0.30078125 loss : 2.0337758\n",
      "670 iters  acc : 0.3359375 loss : 1.9973148\n",
      "671 iters  acc : 0.2890625 loss : 2.0255647\n",
      "672 iters  acc : 0.3125 loss : 2.0149074\n",
      "673 iters  acc : 0.30078125 loss : 2.0221307\n",
      "674 iters  acc : 0.27734375 loss : 2.04907\n",
      "675 iters  acc : 0.30078125 loss : 2.009054\n",
      "676 iters  acc : 0.30078125 loss : 2.0664845\n",
      "677 iters  acc : 0.2421875 loss : 2.1590009\n",
      "678 iters  acc : 0.28125 loss : 2.083489\n",
      "679 iters  acc : 0.35546875 loss : 1.992934\n",
      "680 iters  acc : 0.3671875 loss : 1.9396641\n",
      "681 iters  acc : 0.28125 loss : 2.128572\n",
      "682 iters  acc : 0.25 loss : 2.0832505\n",
      "683 iters  acc : 0.30859375 loss : 2.0367975\n",
      "684 iters  acc : 0.33984375 loss : 1.9142036\n",
      "685 iters  acc : 0.33984375 loss : 1.9839807\n",
      "686 iters  acc : 0.3515625 loss : 1.9576962\n",
      "687 iters  acc : 0.28125 loss : 2.1257823\n",
      "688 iters  acc : 0.30078125 loss : 2.0579457\n",
      "689 iters  acc : 0.33984375 loss : 1.980538\n",
      "690 iters  acc : 0.35546875 loss : 1.9042075\n",
      "691 iters  acc : 0.30078125 loss : 2.0708652\n",
      "692 iters  acc : 0.3125 loss : 2.0423777\n",
      "693 iters  acc : 0.33203125 loss : 2.0198057\n",
      "694 iters  acc : 0.34765625 loss : 2.001477\n",
      "695 iters  acc : 0.26953125 loss : 2.043295\n",
      "696 iters  acc : 0.33984375 loss : 1.9334085\n",
      "697 iters  acc : 0.3203125 loss : 1.981467\n",
      "698 iters  acc : 0.28515625 loss : 2.092717\n",
      "699 iters  acc : 0.32421875 loss : 2.0137858\n",
      "700 iters  acc : 0.359375 loss : 1.9368231\n",
      "701 iters  acc : 0.3125 loss : 2.0234184\n",
      "702 iters  acc : 0.25390625 loss : 2.1798325\n",
      "703 iters  acc : 0.33203125 loss : 2.0465002\n",
      "704 iters  acc : 0.296875 loss : 2.006063\n",
      "705 iters  acc : 0.32421875 loss : 1.9666467\n",
      "706 iters  acc : 0.31640625 loss : 2.0142126\n",
      "707 iters  acc : 0.32421875 loss : 1.9821086\n",
      "708 iters  acc : 0.3359375 loss : 1.9984813\n",
      "709 iters  acc : 0.3515625 loss : 1.9310906\n",
      "710 iters  acc : 0.34765625 loss : 1.9242632\n",
      "711 iters  acc : 0.2578125 loss : 2.1026306\n",
      "712 iters  acc : 0.32421875 loss : 1.9798343\n",
      "713 iters  acc : 0.2890625 loss : 2.0505629\n",
      "714 iters  acc : 0.3125 loss : 2.0587928\n",
      "715 iters  acc : 0.30078125 loss : 1.9812853\n",
      "716 iters  acc : 0.37109375 loss : 1.9120328\n",
      "717 iters  acc : 0.33984375 loss : 1.9688115\n",
      "718 iters  acc : 0.2890625 loss : 2.0443518\n",
      "719 iters  acc : 0.34765625 loss : 1.8946624\n",
      "720 iters  acc : 0.29296875 loss : 2.039378\n",
      "721 iters  acc : 0.3359375 loss : 2.0092874\n",
      "722 iters  acc : 0.23046875 loss : 2.334475\n",
      "723 iters  acc : 0.3203125 loss : 2.039495\n",
      "724 iters  acc : 0.21875 loss : 2.1224809\n",
      "725 iters  acc : 0.328125 loss : 1.9997375\n",
      "726 iters  acc : 0.3671875 loss : 1.9132922\n",
      "727 iters  acc : 0.296875 loss : 1.9980911\n",
      "728 iters  acc : 0.296875 loss : 1.9994332\n",
      "729 iters  acc : 0.2890625 loss : 1.997285\n",
      "730 iters  acc : 0.33203125 loss : 1.9388701\n",
      "731 iters  acc : 0.29296875 loss : 2.0766068\n",
      "732 iters  acc : 0.3359375 loss : 1.8913225\n",
      "733 iters  acc : 0.36328125 loss : 1.8785429\n",
      "734 iters  acc : 0.3515625 loss : 1.922909\n",
      "735 iters  acc : 0.29296875 loss : 2.035969\n",
      "736 iters  acc : 0.36328125 loss : 1.9375143\n",
      "737 iters  acc : 0.26953125 loss : 2.0852218\n",
      "738 iters  acc : 0.328125 loss : 2.004327\n",
      "739 iters  acc : 0.30859375 loss : 2.0120168\n",
      "740 iters  acc : 0.29296875 loss : 2.0336125\n",
      "741 iters  acc : 0.3515625 loss : 1.9060204\n",
      "742 iters  acc : 0.35546875 loss : 1.9476092\n",
      "743 iters  acc : 0.2890625 loss : 2.1358795\n",
      "744 iters  acc : 0.265625 loss : 2.1731844\n",
      "745 iters  acc : 0.31640625 loss : 2.0339608\n",
      "746 iters  acc : 0.3359375 loss : 1.9566288\n",
      "747 iters  acc : 0.359375 loss : 1.9339559\n",
      "748 iters  acc : 0.3515625 loss : 1.9602165\n",
      "749 iters  acc : 0.3203125 loss : 1.9957502\n",
      "750 iters  acc : 0.33984375 loss : 1.9382277\n",
      "751 iters  acc : 0.34765625 loss : 1.9495049\n",
      "752 iters  acc : 0.328125 loss : 1.982691\n",
      "753 iters  acc : 0.375 loss : 1.9176495\n",
      "754 iters  acc : 0.28515625 loss : 2.0230472\n",
      "755 iters  acc : 0.375 loss : 1.9093343\n",
      "756 iters  acc : 0.26953125 loss : 2.1311564\n",
      "757 iters  acc : 0.28515625 loss : 2.0733826\n",
      "758 iters  acc : 0.36328125 loss : 1.9609845\n",
      "759 iters  acc : 0.359375 loss : 1.9136121\n",
      "760 iters  acc : 0.359375 loss : 1.898818\n",
      "761 iters  acc : 0.3125 loss : 2.0282042\n",
      "762 iters  acc : 0.32421875 loss : 2.0461755\n",
      "763 iters  acc : 0.29296875 loss : 2.05897\n",
      "764 iters  acc : 0.265625 loss : 2.0638866\n",
      "765 iters  acc : 0.30859375 loss : 2.022869\n",
      "766 iters  acc : 0.32421875 loss : 2.0458097\n",
      "767 iters  acc : 0.296875 loss : 2.040245\n",
      "768 iters  acc : 0.36328125 loss : 1.8662589\n",
      "769 iters  acc : 0.36328125 loss : 1.915031\n",
      "770 iters  acc : 0.27734375 loss : 2.1306539\n",
      "771 iters  acc : 0.28125 loss : 2.0674512\n",
      "772 iters  acc : 0.35546875 loss : 1.9748439\n",
      "773 iters  acc : 0.38671875 loss : 1.9198034\n",
      "774 iters  acc : 0.359375 loss : 1.8996991\n",
      "775 iters  acc : 0.2734375 loss : 2.062992\n",
      "776 iters  acc : 0.3359375 loss : 1.9482763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777 iters  acc : 0.40234375 loss : 1.864059\n",
      "778 iters  acc : 0.37109375 loss : 1.9412385\n",
      "779 iters  acc : 0.29296875 loss : 2.115102\n",
      "780 iters  acc : 0.29296875 loss : 2.02964\n",
      "781 iters  acc : 0.390625 loss : 1.8715137\n",
      "782 iters  acc : 0.375 loss : 1.9477649\n",
      "783 iters  acc : 0.3203125 loss : 2.0228922\n",
      "784 iters  acc : 0.3828125 loss : 1.9446387\n",
      "785 iters  acc : 0.3203125 loss : 2.0300448\n",
      "786 iters  acc : 0.30078125 loss : 1.9961735\n",
      "787 iters  acc : 0.31640625 loss : 1.9823799\n",
      "788 iters  acc : 0.3515625 loss : 1.9785016\n",
      "789 iters  acc : 0.34375 loss : 1.9616787\n",
      "790 iters  acc : 0.34765625 loss : 1.9652078\n",
      "791 iters  acc : 0.3046875 loss : 1.9733183\n",
      "792 iters  acc : 0.33203125 loss : 1.9824631\n",
      "793 iters  acc : 0.328125 loss : 1.9911886\n",
      "794 iters  acc : 0.3125 loss : 1.9599009\n",
      "795 iters  acc : 0.32421875 loss : 1.962339\n",
      "796 iters  acc : 0.3046875 loss : 2.0009904\n",
      "797 iters  acc : 0.3125 loss : 2.0498922\n",
      "798 iters  acc : 0.33984375 loss : 1.9917492\n",
      "799 iters  acc : 0.22265625 loss : 2.2064714\n",
      "800 iters  acc : 0.28125 loss : 2.094893\n",
      "801 iters  acc : 0.3828125 loss : 1.9596624\n",
      "802 iters  acc : 0.3515625 loss : 1.9441934\n",
      "803 iters  acc : 0.2734375 loss : 2.05027\n",
      "804 iters  acc : 0.30859375 loss : 1.9770563\n",
      "805 iters  acc : 0.3828125 loss : 1.915035\n",
      "806 iters  acc : 0.33203125 loss : 1.947604\n",
      "807 iters  acc : 0.3046875 loss : 1.9952763\n",
      "808 iters  acc : 0.33203125 loss : 2.0110466\n",
      "809 iters  acc : 0.2734375 loss : 2.0823145\n",
      "810 iters  acc : 0.2734375 loss : 2.0583909\n",
      "811 iters  acc : 0.40234375 loss : 1.8512369\n",
      "812 iters  acc : 0.328125 loss : 1.9467505\n",
      "813 iters  acc : 0.31640625 loss : 2.0492923\n",
      "814 iters  acc : 0.3125 loss : 2.0073547\n",
      "815 iters  acc : 0.29296875 loss : 2.046076\n",
      "816 iters  acc : 0.31640625 loss : 2.0416079\n",
      "817 iters  acc : 0.28515625 loss : 2.0546818\n",
      "818 iters  acc : 0.28125 loss : 2.085143\n",
      "819 iters  acc : 0.28515625 loss : 2.0171638\n",
      "820 iters  acc : 0.33984375 loss : 1.9589815\n",
      "821 iters  acc : 0.3203125 loss : 1.9472175\n",
      "822 iters  acc : 0.35546875 loss : 1.937499\n",
      "823 iters  acc : 0.359375 loss : 1.9653652\n",
      "824 iters  acc : 0.34375 loss : 2.0489204\n",
      "825 iters  acc : 0.29296875 loss : 2.1435091\n",
      "826 iters  acc : 0.3359375 loss : 2.051374\n",
      "827 iters  acc : 0.34765625 loss : 1.9428698\n",
      "828 iters  acc : 0.34375 loss : 1.9488266\n",
      "829 iters  acc : 0.31640625 loss : 1.9034127\n",
      "830 iters  acc : 0.3203125 loss : 1.9343494\n",
      "831 iters  acc : 0.32421875 loss : 1.9906659\n",
      "832 iters  acc : 0.3046875 loss : 1.9696279\n",
      "833 iters  acc : 0.3359375 loss : 1.9200721\n",
      "834 iters  acc : 0.3125 loss : 2.0126348\n",
      "835 iters  acc : 0.3828125 loss : 1.85011\n",
      "836 iters  acc : 0.34375 loss : 1.9173841\n",
      "837 iters  acc : 0.33203125 loss : 1.9571145\n",
      "838 iters  acc : 0.32421875 loss : 1.9324206\n",
      "839 iters  acc : 0.28515625 loss : 2.1400476\n",
      "840 iters  acc : 0.31640625 loss : 1.990452\n",
      "841 iters  acc : 0.359375 loss : 1.9777212\n",
      "842 iters  acc : 0.30859375 loss : 1.9849985\n",
      "843 iters  acc : 0.2890625 loss : 2.0402732\n",
      "844 iters  acc : 0.3828125 loss : 1.9359286\n",
      "845 iters  acc : 0.33203125 loss : 1.9723234\n",
      "846 iters  acc : 0.3515625 loss : 1.9235934\n",
      "847 iters  acc : 0.28125 loss : 2.0242133\n",
      "848 iters  acc : 0.35546875 loss : 1.9427614\n",
      "849 iters  acc : 0.3671875 loss : 1.9197745\n",
      "850 iters  acc : 0.3671875 loss : 1.8710082\n",
      "851 iters  acc : 0.32421875 loss : 1.9855149\n",
      "852 iters  acc : 0.34765625 loss : 1.9598591\n",
      "853 iters  acc : 0.3125 loss : 2.048174\n",
      "854 iters  acc : 0.3125 loss : 2.005869\n",
      "855 iters  acc : 0.33203125 loss : 1.92177\n",
      "856 iters  acc : 0.31640625 loss : 1.9673994\n",
      "857 iters  acc : 0.3828125 loss : 1.8357898\n",
      "858 iters  acc : 0.3046875 loss : 2.0376713\n",
      "859 iters  acc : 0.3359375 loss : 1.9632366\n",
      "860 iters  acc : 0.33984375 loss : 1.9331187\n",
      "861 iters  acc : 0.31640625 loss : 2.0026069\n",
      "862 iters  acc : 0.2890625 loss : 2.0763664\n",
      "863 iters  acc : 0.3359375 loss : 1.9750899\n",
      "864 iters  acc : 0.35546875 loss : 1.9538\n",
      "865 iters  acc : 0.34765625 loss : 1.8926528\n",
      "866 iters  acc : 0.3671875 loss : 1.9751239\n",
      "867 iters  acc : 0.3203125 loss : 2.002208\n",
      "868 iters  acc : 0.2578125 loss : 2.065773\n",
      "869 iters  acc : 0.2421875 loss : 2.0835595\n",
      "870 iters  acc : 0.35546875 loss : 1.9981881\n",
      "871 iters  acc : 0.34375 loss : 1.905837\n",
      "872 iters  acc : 0.33984375 loss : 1.9513571\n",
      "873 iters  acc : 0.30078125 loss : 2.0370479\n",
      "874 iters  acc : 0.34375 loss : 1.9251535\n",
      "875 iters  acc : 0.328125 loss : 1.9309528\n",
      "876 iters  acc : 0.4140625 loss : 1.8480145\n",
      "877 iters  acc : 0.3515625 loss : 1.9252317\n",
      "878 iters  acc : 0.34765625 loss : 1.9472926\n",
      "879 iters  acc : 0.375 loss : 1.8868121\n",
      "880 iters  acc : 0.33984375 loss : 1.9303833\n",
      "881 iters  acc : 0.2734375 loss : 2.026659\n",
      "882 iters  acc : 0.34375 loss : 1.9718139\n",
      "883 iters  acc : 0.3203125 loss : 1.9982939\n",
      "884 iters  acc : 0.3515625 loss : 1.9408665\n",
      "885 iters  acc : 0.34375 loss : 1.9641045\n",
      "886 iters  acc : 0.3671875 loss : 1.8916932\n",
      "887 iters  acc : 0.33203125 loss : 1.9697239\n",
      "888 iters  acc : 0.34375 loss : 1.9038401\n",
      "889 iters  acc : 0.359375 loss : 1.9614803\n",
      "890 iters  acc : 0.31640625 loss : 1.9830151\n",
      "891 iters  acc : 0.33984375 loss : 1.963628\n",
      "892 iters  acc : 0.3046875 loss : 2.1237183\n",
      "893 iters  acc : 0.26171875 loss : 2.150165\n",
      "894 iters  acc : 0.30859375 loss : 2.0243187\n",
      "895 iters  acc : 0.33984375 loss : 1.9467611\n",
      "896 iters  acc : 0.3515625 loss : 1.9620907\n",
      "897 iters  acc : 0.3671875 loss : 1.9212205\n",
      "898 iters  acc : 0.37890625 loss : 1.9669396\n",
      "899 iters  acc : 0.3515625 loss : 1.91032\n",
      "900 iters  acc : 0.37890625 loss : 1.8744831\n",
      "901 iters  acc : 0.33203125 loss : 1.9283124\n",
      "902 iters  acc : 0.28515625 loss : 2.082569\n",
      "903 iters  acc : 0.27734375 loss : 2.071043\n",
      "904 iters  acc : 0.3125 loss : 1.9928396\n",
      "905 iters  acc : 0.33203125 loss : 1.9616939\n",
      "906 iters  acc : 0.3359375 loss : 2.0028195\n",
      "907 iters  acc : 0.28515625 loss : 2.0207593\n",
      "908 iters  acc : 0.390625 loss : 1.8487432\n",
      "909 iters  acc : 0.37109375 loss : 1.8532331\n",
      "910 iters  acc : 0.31640625 loss : 2.0445352\n",
      "911 iters  acc : 0.3203125 loss : 1.9987744\n",
      "912 iters  acc : 0.3359375 loss : 1.9458551\n",
      "913 iters  acc : 0.3515625 loss : 1.9083626\n",
      "914 iters  acc : 0.37109375 loss : 1.9079496\n",
      "915 iters  acc : 0.31640625 loss : 2.0190015\n",
      "916 iters  acc : 0.27734375 loss : 2.1237187\n",
      "917 iters  acc : 0.2578125 loss : 2.0937786\n",
      "918 iters  acc : 0.359375 loss : 1.9745748\n",
      "919 iters  acc : 0.33984375 loss : 1.9501995\n",
      "920 iters  acc : 0.3515625 loss : 1.9060907\n",
      "921 iters  acc : 0.328125 loss : 1.8879998\n",
      "922 iters  acc : 0.33203125 loss : 1.8947575\n",
      "923 iters  acc : 0.30078125 loss : 1.9782958\n",
      "924 iters  acc : 0.28515625 loss : 1.991518\n",
      "925 iters  acc : 0.359375 loss : 1.9591529\n",
      "926 iters  acc : 0.375 loss : 1.8335426\n",
      "927 iters  acc : 0.359375 loss : 1.9198531\n",
      "928 iters  acc : 0.34375 loss : 1.9608897\n",
      "929 iters  acc : 0.34765625 loss : 1.9496562\n",
      "930 iters  acc : 0.3125 loss : 1.9705347\n",
      "931 iters  acc : 0.35546875 loss : 1.9324915\n",
      "932 iters  acc : 0.30078125 loss : 1.9958084\n",
      "933 iters  acc : 0.33984375 loss : 1.8611848\n",
      "934 iters  acc : 0.37890625 loss : 1.9104636\n",
      "935 iters  acc : 0.30078125 loss : 2.0154023\n",
      "936 iters  acc : 0.375 loss : 1.9285378\n",
      "937 iters  acc : 0.34765625 loss : 1.9876516\n",
      "938 iters  acc : 0.3828125 loss : 1.8656256\n",
      "939 iters  acc : 0.3203125 loss : 1.9821872\n",
      "940 iters  acc : 0.36328125 loss : 1.9069575\n",
      "941 iters  acc : 0.3046875 loss : 2.0096667\n",
      "942 iters  acc : 0.3515625 loss : 1.9847876\n",
      "943 iters  acc : 0.35546875 loss : 1.8750663\n",
      "944 iters  acc : 0.2890625 loss : 2.0264082\n",
      "945 iters  acc : 0.34375 loss : 1.9878191\n",
      "946 iters  acc : 0.34375 loss : 1.9184866\n",
      "947 iters  acc : 0.3203125 loss : 1.9396646\n",
      "948 iters  acc : 0.30078125 loss : 1.9929382\n",
      "949 iters  acc : 0.3125 loss : 1.9929516\n",
      "950 iters  acc : 0.3828125 loss : 1.8496797\n",
      "951 iters  acc : 0.2890625 loss : 2.0541208\n",
      "952 iters  acc : 0.3828125 loss : 1.9051476\n",
      "953 iters  acc : 0.34375 loss : 1.9162637\n",
      "954 iters  acc : 0.40625 loss : 1.8608811\n",
      "955 iters  acc : 0.34375 loss : 2.0014167\n",
      "956 iters  acc : 0.3125 loss : 1.9645752\n",
      "957 iters  acc : 0.33984375 loss : 1.9663057\n",
      "958 iters  acc : 0.32421875 loss : 1.9773413\n",
      "959 iters  acc : 0.296875 loss : 2.0211103\n",
      "960 iters  acc : 0.3515625 loss : 1.922591\n",
      "961 iters  acc : 0.33203125 loss : 1.9703746\n",
      "962 iters  acc : 0.32421875 loss : 1.9988256\n",
      "963 iters  acc : 0.3359375 loss : 1.9882476\n",
      "964 iters  acc : 0.30859375 loss : 1.9657304\n",
      "965 iters  acc : 0.36328125 loss : 1.8845904\n",
      "966 iters  acc : 0.33203125 loss : 1.9619474\n",
      "967 iters  acc : 0.32421875 loss : 2.080083\n",
      "968 iters  acc : 0.34375 loss : 1.9544505\n",
      "969 iters  acc : 0.33203125 loss : 1.962802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 iters  acc : 0.30859375 loss : 2.03226\n",
      "971 iters  acc : 0.38671875 loss : 1.9148681\n",
      "972 iters  acc : 0.328125 loss : 1.9568771\n",
      "973 iters  acc : 0.3984375 loss : 1.8631046\n",
      "974 iters  acc : 0.3515625 loss : 1.9338884\n",
      "975 iters  acc : 0.3671875 loss : 1.9171462\n",
      "976 iters  acc : 0.3125 loss : 2.0525246\n",
      "977 iters  acc : 0.2734375 loss : 2.0769107\n",
      "978 iters  acc : 0.3203125 loss : 2.0126138\n",
      "979 iters  acc : 0.34765625 loss : 1.9314556\n",
      "980 iters  acc : 0.3515625 loss : 1.9828715\n",
      "981 iters  acc : 0.4375 loss : 1.8137851\n",
      "982 iters  acc : 0.37890625 loss : 1.9176309\n",
      "983 iters  acc : 0.34765625 loss : 1.9415169\n",
      "984 iters  acc : 0.3203125 loss : 1.9953345\n",
      "985 iters  acc : 0.34765625 loss : 1.9456669\n",
      "986 iters  acc : 0.34375 loss : 1.8968602\n",
      "987 iters  acc : 0.359375 loss : 1.9439062\n",
      "988 iters  acc : 0.31640625 loss : 1.9299304\n",
      "989 iters  acc : 0.27734375 loss : 2.0177681\n",
      "990 iters  acc : 0.33984375 loss : 1.9414442\n",
      "991 iters  acc : 0.3046875 loss : 1.9936533\n",
      "992 iters  acc : 0.3125 loss : 1.9592192\n",
      "993 iters  acc : 0.3515625 loss : 1.8766421\n",
      "994 iters  acc : 0.37890625 loss : 1.9006497\n",
      "995 iters  acc : 0.30859375 loss : 2.0559158\n",
      "996 iters  acc : 0.36328125 loss : 1.9479723\n",
      "997 iters  acc : 0.3515625 loss : 1.9219965\n",
      "998 iters  acc : 0.32421875 loss : 1.9904155\n",
      "999 iters  acc : 0.38671875 loss : 1.8559396\n",
      "23.228970527648926\n"
     ]
    }
   ],
   "source": [
    "r_index = np.arange(train_x.shape[0])\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    batch_idx = np.random.choice(r_index,256,replace=False)\n",
    "    batch_x , batch_y = train_x[batch_idx] , train_y[batch_idx]\n",
    "\n",
    "    _,s_acc,s_loss = sess.run([apply_gradient_op,mean_acc,mean_loss],feed_dict={img:batch_x,ys:batch_y})\n",
    "    print(i,\"iters \",\"acc :\",s_acc,\"loss :\" ,s_loss)\n",
    "    \n",
    "print(time.time()-start_time)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Parallel computing and using two gpu to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_g = tf.Graph()\n",
    "with cifar_g.as_default() as g , tf.device(\"/gpu:0\"):\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        img = tf.placeholder(tf.float32,[None,32,32,3])\n",
    "        ys = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        split_img = tf.split(img,2)\n",
    "        split_label = tf.split(ys,2)\n",
    "        \n",
    "    with tf.name_scope(\"train_optimize\"):\n",
    "        optimizer = tf.train.RMSPropOptimizer(5e-4)\n",
    "        \n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        x = build_model_1(img,0)\n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        score = build_model_2(x,1)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=score,labels=ys)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        prob  = tf.nn.softmax(score,axis=-1)\n",
    "        acc = tf.argmax(prob,axis=-1,output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(acc,ys),tf.float32))\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(5e-4).minimize(loss)\n",
    "    \n",
    "    tf.summary.FileWriter(logdir=\"tblog/cifar1\",graph=g)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "print(\"Build...\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=cifar_g,config=sess_opt)\n",
    "sess.run(init)\n",
    "\n",
    "r_index = np.arange(train_x.shape[0])\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    batch_idx = np.random.choice(r_index,256,replace=False)\n",
    "    batch_x , batch_y = train_x[batch_idx] , train_y[batch_idx]\n",
    "\n",
    "    _,s_acc,s_loss = sess.run([opt,acc,loss],feed_dict={img:batch_x,ys:batch_y})\n",
    "    print(i,\"iters \",\"acc :\",s_acc,\"loss :\" ,s_loss)\n",
    "    \n",
    "print(time.time()-start_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(mean_acc,feed_dict={img:val_x,ys:val_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.12",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
